{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting environment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will continue working on the Kickstarter data we cleaned up and evaluate with different algorithm such as decision tree and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backers_count</th>\n",
       "      <th>disable_communication</th>\n",
       "      <th>goal</th>\n",
       "      <th>is_starrable</th>\n",
       "      <th>staff_pick</th>\n",
       "      <th>state</th>\n",
       "      <th>name_Action</th>\n",
       "      <th>name_Animals</th>\n",
       "      <th>name_Audio</th>\n",
       "      <th>name_Children's Books</th>\n",
       "      <th>...</th>\n",
       "      <th>color_2577151</th>\n",
       "      <th>color_51627</th>\n",
       "      <th>color_58341</th>\n",
       "      <th>color_6526716</th>\n",
       "      <th>location_AU</th>\n",
       "      <th>location_CA</th>\n",
       "      <th>location_DE</th>\n",
       "      <th>location_GB</th>\n",
       "      <th>location_Others</th>\n",
       "      <th>location_US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   backers_count  disable_communication     goal  is_starrable  staff_pick  \\\n",
       "0            170                      0  25000.0             0           0   \n",
       "1             10                      0    500.0             0           0   \n",
       "2              0                      0   3500.0             0           0   \n",
       "3              0                      0  20000.0             0           0   \n",
       "4             62                      0   5000.0             0           1   \n",
       "\n",
       "   state  name_Action  name_Animals  name_Audio  name_Children's Books  \\\n",
       "0      1            0             0           0                      0   \n",
       "1      1            0             0           0                      1   \n",
       "2      0            0             0           1                      0   \n",
       "3      0            0             0           0                      0   \n",
       "4      1            0             0           0                      0   \n",
       "\n",
       "      ...       color_2577151  color_51627  color_58341  color_6526716  \\\n",
       "0     ...                   0            0            0              0   \n",
       "1     ...                   0            0            0              0   \n",
       "2     ...                   0            0            0              0   \n",
       "3     ...                   0            0            0              0   \n",
       "4     ...                   0            0            0              0   \n",
       "\n",
       "   location_AU  location_CA  location_DE  location_GB  location_Others  \\\n",
       "0            0            0            0            0                0   \n",
       "1            0            0            0            0                0   \n",
       "2            0            1            0            0                0   \n",
       "3            0            0            0            0                0   \n",
       "4            0            0            0            0                0   \n",
       "\n",
       "   location_US  \n",
       "0            1  \n",
       "1            1  \n",
       "2            0  \n",
       "3            1  \n",
       "4            1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "KS_clean = pd.read_csv('data/KS_clean.csv')\n",
    "KS_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fitting our model, we will split the training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(KS_clean.drop('state', axis=1), KS_clean['state'], test_size=0.3,\n",
    "                                                   random_state=1009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8540209790209791\n"
     ]
    }
   ],
   "source": [
    "# simple approach, set up random parameters to see how decision tree works\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "dst = DecisionTreeClassifier(criterion='entropy', max_features=5, max_depth=5, random_state=1009)\n",
    "dst.fit(X_train, y_train)\n",
    "\n",
    "prediction = dst.predict(X_test)\n",
    "# compare the real data with prediction\n",
    "print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see by randomly select our parameters, we get a score of 85.4% accuracy, which is pretty good, let's see how it performs in other error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 0.8811544991511036\n",
      "False positive rate: 0.17477477477477477\n"
     ]
    }
   ],
   "source": [
    "# calculate true positive rate and false positive rate\n",
    "tp = (prediction == 1) & (y_test == 1)\n",
    "tn = (prediction == 0) & (y_test == 0)\n",
    "fp = (prediction == 1) & (y_test == 0)\n",
    "fn = (prediction == 0) & (y_test == 1)\n",
    "print('True positive rate: {}'.format(np.sum(tp)/(np.sum(tp)+np.sum(fn))))\n",
    "print('False positive rate: {}'.format(np.sum(fp)/(np.sum(fp)+np.sum(tn))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our first approach of decision tree algorithms performs pretty well, and it is better than Naive Bayes we did previously. Our next step is to find out the best tree model. there are many parameters to tune to find the best combination, such as `criterion`, `splitter`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`..etc. We will use GridSearch to find the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# setting hyperparameters\n",
    "hyperparameters = {\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'max_depth': range(3,11),\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'min_samples_leaf': range(1,11),\n",
    "    'min_samples_split': [3, 5],\n",
    "    }\n",
    "\n",
    "dst_2 = DecisionTreeClassifier(random_state=1009)\n",
    "grid = GridSearchCV(dst_2, param_grid=hyperparameters, cv=10)\n",
    "\n",
    "# use training set to find the best parameter\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8907342657342657\n"
     ]
    }
   ],
   "source": [
    "# use the best parameters above to test our model\n",
    "dst_3 = DecisionTreeClassifier(criterion='gini', max_depth=10, max_features='sqrt', min_samples_leaf=1,\n",
    "                               min_samples_split=5, random_state=1009)\n",
    "dst_3.fit(X_train, y_train)\n",
    "\n",
    "prediction = dst_3.predict(X_test)\n",
    "# compare the real data with prediction\n",
    "print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 0.9151103565365025\n",
      "False positive rate: 0.13513513513513514\n"
     ]
    }
   ],
   "source": [
    "# calculate true positive rate and false positive rate\n",
    "tp = (prediction == 1) & (y_test == 1)\n",
    "tn = (prediction == 0) & (y_test == 0)\n",
    "fp = (prediction == 1) & (y_test == 0)\n",
    "fn = (prediction == 0) & (y_test == 1)\n",
    "print('True positive rate: {}'.format(np.sum(tp)/(np.sum(tp)+np.sum(fn))))\n",
    "print('False positive rate: {}'.format(np.sum(fp)/(np.sum(fp)+np.sum(tn))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some improvements by tuning our model, the next step is to use cross validation to see if there's any overfitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88219895, 0.92408377, 0.90551181, 0.90026247, 0.89501312,\n",
       "       0.90551181, 0.8687664 , 0.88713911, 0.89238845, 0.91052632])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(dst_3, KS_clean.drop(['state'], axis=1), KS_clean['state'], cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5}\n"
     ]
    }
   ],
   "source": [
    "# let's try to fit with the whole dataset using Gridsearch\n",
    "dst_4 = DecisionTreeClassifier(random_state=1009)\n",
    "grid = GridSearchCV(dst_4, param_grid=hyperparameters, cv=10)\n",
    "\n",
    "# use training set to find the best parameter\n",
    "grid.fit(KS_clean.drop(['state'], axis=1), KS_clean['state'])\n",
    "\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8784965034965035\n"
     ]
    }
   ],
   "source": [
    "dst_5 = DecisionTreeClassifier(criterion='entropy', max_depth=10, max_features='log2', min_samples_leaf=1,\n",
    "                               min_samples_split=5, random_state=1009)\n",
    "dst_5.fit(X_train, y_train)\n",
    "\n",
    "prediction = dst_5.predict(X_test)\n",
    "# compare the real data with prediction\n",
    "print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 0.9303904923599321\n",
      "False positive rate: 0.17657657657657658\n"
     ]
    }
   ],
   "source": [
    "# calculate true positive rate and false positive rate\n",
    "tp = (prediction == 1) & (y_test == 1)\n",
    "tn = (prediction == 0) & (y_test == 0)\n",
    "fp = (prediction == 1) & (y_test == 0)\n",
    "fn = (prediction == 0) & (y_test == 1)\n",
    "print('True positive rate: {}'.format(np.sum(tp)/(np.sum(tp)+np.sum(fn))))\n",
    "print('False positive rate: {}'.format(np.sum(fp)/(np.sum(fp)+np.sum(tn))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92408377, 0.91361257, 0.92388451, 0.89238845, 0.9160105 ,\n",
       "       0.90551181, 0.86351706, 0.90551181, 0.8687664 , 0.91578947])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(dst_5, KS_clean.drop(['state'], axis=1), KS_clean['state'], cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From testing different approaches previously, we will use the dst_3 parameters as our best model for decision tree, next we will compare them with random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9379370629370629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# we will use the simples model without specifying any parameter\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "prediction = rf.predict(X_test)\n",
    "# compare the real data with prediction\n",
    "print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 0.9371816638370118\n",
      "False positive rate: 0.06126126126126126\n"
     ]
    }
   ],
   "source": [
    "# calculate true positive rate and false positive rate\n",
    "tp = (prediction == 1) & (y_test == 1)\n",
    "tn = (prediction == 0) & (y_test == 0)\n",
    "fp = (prediction == 1) & (y_test == 0)\n",
    "fn = (prediction == 0) & (y_test == 1)\n",
    "print('True positive rate: {}'.format(np.sum(tp)/(np.sum(tp)+np.sum(fn))))\n",
    "print('False positive rate: {}'.format(np.sum(fp)/(np.sum(fp)+np.sum(tn))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 398 ms, sys: 2.21 ms, total: 400 ms\n",
      "Wall time: 400 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92670157, 0.95026178, 0.95013123, 0.91863517, 0.93963255,\n",
       "       0.94488189, 0.93175853, 0.95013123, 0.92913386, 0.93947368])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf, KS_clean.drop(['state'], axis=1), KS_clean['state'], cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even without tuning any parameters in random forest algorithm, it still performs better than our best decision tree model in every aspect, since random forest takes the average of many decision trees, even a tree in the forest falls, it can still outperform a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will compare the time it takes to find a best decision tree parameters v.s. the time it takes to use a simplest random tree algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.8907342657342657\n",
      "CPU times: user 32.5 s, sys: 267 ms, total: 32.7 s\n",
      "Wall time: 33.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hyperparameters = {\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'max_depth': range(3,11),\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'min_samples_leaf': range(1,11),\n",
    "    'min_samples_split': [3, 5],\n",
    "    }\n",
    "\n",
    "dst_2 = DecisionTreeClassifier(random_state=1009)\n",
    "grid = GridSearchCV(dst_2, param_grid=hyperparameters, cv=10)\n",
    "\n",
    "# use training set to find the best parameter\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "dst_3 = DecisionTreeClassifier(criterion='gini', max_depth=10, max_features='sqrt', min_samples_leaf=1,\n",
    "                               min_samples_split=5, random_state=1009)\n",
    "dst_3.fit(X_train, y_train)\n",
    "\n",
    "prediction = dst_3.predict(X_test)\n",
    "# compare the real data with prediction\n",
    "print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9335664335664335\n",
      "CPU times: user 36.3 ms, sys: 1.28 ms, total: 37.6 ms\n",
      "Wall time: 36.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "prediction = rf.predict(X_test)\n",
    "# compare the real data with prediction\n",
    "print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the complexity between two models are pretty huge, almost around 1000 folds! We then jump into the conclusion that even the simplest random forest model outperforms the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
